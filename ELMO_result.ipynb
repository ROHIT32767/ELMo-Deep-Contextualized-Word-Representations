{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7v0szo8G1wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2e4585-b24d-43d1-8bf4-76afb7becec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "!pip3 install contractions\n",
        "import contractions\n",
        "import string\n",
        "import re\n",
        "import torch\n",
        "import sys\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class create_dataset(Dataset):\n",
        "    def __init__(self, data_path, threshold=3, vocab=None, word2idx=None, idx2word=None):\n",
        "        self.data_path = data_path\n",
        "        self.threshold = threshold\n",
        "        self.sentences = None\n",
        "        self.vocab = vocab\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.max_length = None\n",
        "        self.X_forward = None\n",
        "        self.X_backward = None\n",
        "        self.y_forward = None\n",
        "        self.y_backward = None\n",
        "        self.preprocess_data()\n",
        "        if vocab is None:\n",
        "            self.create_vocab()\n",
        "        self.get_max_length()\n",
        "        self.padding()\n",
        "        self.create_training_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        data = pd.read_csv(self.data_path)\n",
        "        sentences = data[\"Description\"].values\n",
        "        sentences = [contractions.fix(sentence) for sentence in sentences]\n",
        "        sentences = [sentence.lower() for sentence in sentences]\n",
        "        sentences = [re.sub(r'http\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [re.sub(r'www\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [sentence.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for sentence in sentences]\n",
        "        sentences = [(sentence.split()) for sentence in sentences]\n",
        "        sentences = [['<s>'] + sentence + ['</s>'] for sentence in sentences]\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def create_vocab(self):\n",
        "        words = [word for sentence in self.sentences for word in sentence]\n",
        "        word_freq = Counter(words)\n",
        "        vocab = [word for word, freq in word_freq.items() if freq >= self.threshold]\n",
        "        vocab = ['<pad>', '<unk>'] + vocab\n",
        "        self.vocab = vocab\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def get_max_length(self):\n",
        "        self.max_length = int(self.get_n_percentile_sentence_length(95))\n",
        "\n",
        "    def get_n_percentile_sentence_length(self, percentile):\n",
        "        sentence_lengths = [len(sentence) for sentence in self.sentences]\n",
        "        return np.percentile(sentence_lengths, percentile)\n",
        "\n",
        "    def padding(self):\n",
        "        padded_sentences = []\n",
        "        for sentence in self.sentences:\n",
        "            padded_sentence = [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in sentence]\n",
        "            if len(padded_sentence) < self.max_length:\n",
        "                padded_sentence += [self.word2idx['<pad>']] * int(self.max_length - len(padded_sentence))\n",
        "                padded_sentences.append(padded_sentence)\n",
        "            else:\n",
        "                padded_sentences.append(padded_sentence[:self.max_length])\n",
        "\n",
        "        self.sentences = padded_sentences\n",
        "\n",
        "    def create_training_data(self):\n",
        "        X_forward = []\n",
        "        X_backward = []\n",
        "        y_forward = []\n",
        "        y_backward = []\n",
        "        for sentence in self.sentences:\n",
        "            X_forward.append(sentence[:-1])\n",
        "            X_backward.append(sentence[::-1][:-1])\n",
        "            y_forward.append(sentence[1:])\n",
        "            y_backward.append(sentence[::-1][1:])\n",
        "\n",
        "        self.X_forward = torch.tensor(X_forward)\n",
        "        self.X_backward = torch.tensor(X_backward)\n",
        "        self.y_forward = torch.tensor(y_forward)\n",
        "        self.y_backward = torch.tensor(y_backward)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_forward)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_forward[idx], self.X_backward[idx], self.y_forward[idx], self.y_backward[idx]\n",
        "\n",
        "class Elmo(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Elmo, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_forward = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.fc_backward = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, X_forward, X_backward):\n",
        "        forward_embedding = self.embedding(X_forward)\n",
        "        backward_embedding = self.embedding(X_backward)\n",
        "        forward_lstm1, _ = self.lstm_forward1(forward_embedding)\n",
        "        backward_lstm1, _ = self.lstm_backward1(backward_embedding)\n",
        "        forward_lstm2, _ = self.lstm_forward2(forward_lstm1)\n",
        "        backward_lstm2, _ = self.lstm_backward2(backward_lstm1)\n",
        "        forward_output = self.fc_forward(forward_lstm2)\n",
        "        backward_output = self.fc_backward(backward_lstm2)\n",
        "        return forward_output, backward_output\n",
        "\n",
        "\n",
        "def train_elmo(model, train_loader, device, vocab_size, epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            X_forward, X_backward, y_forward, y_backward = data\n",
        "            X_forward, X_backward, y_forward, y_backward = X_forward.to(device), X_backward.to(device), y_forward.to(device), y_backward.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            forward_output, backward_output = model(X_forward, X_backward)\n",
        "            y_forward_one_hot = torch.nn.functional.one_hot(y_forward, num_classes=vocab_size).float()\n",
        "            y_backward_one_hot = torch.nn.functional.one_hot(y_backward, num_classes=vocab_size).float()\n",
        "            forward_output = forward_output.permute(0, 2, 1)\n",
        "            backward_output = backward_output.permute(0, 2, 1)\n",
        "            y_forward_one_hot = y_forward_one_hot.permute(0, 2, 1)\n",
        "            y_backward_one_hot = y_backward_one_hot.permute(0, 2, 1)\n",
        "            forward_loss = criterion(forward_output, y_forward_one_hot)\n",
        "            backward_loss = criterion(backward_output, y_backward_one_hot)\n",
        "            loss = forward_loss + backward_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        losses.append(running_loss/len(train_loader))\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    return losses, model"
      ],
      "metadata": {
        "id": "n3iQ7pUzG4yh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'data/train.csv'\n",
        "threshold = 3\n",
        "dataset = create_dataset(data_path, threshold)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_data_path = 'data/test.csv'\n",
        "test_dataset = create_dataset(test_data_path, threshold, vocab=dataset.vocab, word2idx=dataset.word2idx, idx2word=dataset.idx2word)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "vocab_size = len(dataset.word2idx)\n",
        "embedding_dim = 150\n",
        "hidden_dim = 150\n",
        "model = Elmo(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "losses, model = train_elmo(model, train_loader, device, vocab_size, epochs=10)\n",
        "\n",
        "torch.save(model, 'model.pt')\n",
        "\n",
        "torch.save(dataset.word2idx, 'word2idx.pt')\n",
        "torch.save(dataset.idx2word, 'idx2word.pt')"
      ],
      "metadata": {
        "id": "Bi1poeruG_jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f2fa78-5cc4-43ff-ff73-b6ec000159ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.426731818771362\n",
            "Epoch 2, Loss: 7.098492701085409\n",
            "Epoch 3, Loss: 6.5969685976664225\n",
            "Epoch 4, Loss: 6.303259781901041\n",
            "Epoch 5, Loss: 6.0989046332041426\n",
            "Epoch 6, Loss: 5.94296741587321\n",
            "Epoch 7, Loss: 5.8170874876658125\n",
            "Epoch 8, Loss: 5.711459554672241\n",
            "Epoch 9, Loss: 5.619714071019491\n",
            "Epoch 10, Loss: 5.540601449966431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_dataset_classification(Dataset):\n",
        "    def __init__(self, data_path, word2idx, idx2word):\n",
        "        self.data_path = data_path\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.sentences = None\n",
        "        self.labels = None\n",
        "        self.num_classes = None\n",
        "        self.max_length = None\n",
        "        self.X = None\n",
        "        self.Y = None\n",
        "        self.preprocess_data()\n",
        "        self.get_max_length()\n",
        "        self.padding()\n",
        "        self.create_training_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        data = pd.read_csv(self.data_path)\n",
        "        sentences = data[\"Description\"].values\n",
        "        self.labels = data[\"Class Index\"].values\n",
        "        self.labels = [label - 1 for label in self.labels]\n",
        "        self.num_classes = len(set(self.labels))\n",
        "        self.labels = torch.nn.functional.one_hot(torch.tensor(self.labels), num_classes=self.num_classes).float()\n",
        "        sentences = [contractions.fix(sentence) for sentence in sentences]\n",
        "        sentences = [sentence.lower() for sentence in sentences]\n",
        "        sentences = [re.sub(r'http\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [re.sub(r'www\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [sentence.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for sentence in sentences]\n",
        "        sentences = [(sentence.split()) for sentence in sentences]\n",
        "        sentences = [['<s>'] + sentence + ['</s>'] for sentence in sentences]\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def get_max_length(self):\n",
        "        self.max_length = int(self.get_n_percentile_sentence_length(95))\n",
        "\n",
        "    def get_n_percentile_sentence_length(self, percentile):\n",
        "        sentence_lengths = [len(sentence) for sentence in self.sentences]\n",
        "        return np.percentile(sentence_lengths, percentile)\n",
        "\n",
        "    def padding(self):\n",
        "        padded_sentences = []\n",
        "        for sentence in self.sentences:\n",
        "            padded_sentence = [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in sentence]\n",
        "            if len(padded_sentence) < self.max_length:\n",
        "                padded_sentence += [self.word2idx['<pad>']] * int(self.max_length - len(padded_sentence))\n",
        "                padded_sentences.append(padded_sentence)\n",
        "            else:\n",
        "                padded_sentences.append(padded_sentence[:self.max_length])\n",
        "\n",
        "        self.sentences = padded_sentences\n",
        "\n",
        "    def create_training_data(self):\n",
        "        X = []\n",
        "        for sentence in self.sentences:\n",
        "            X.append(sentence)\n",
        "\n",
        "        self.X = torch.tensor(X)\n",
        "        self.Y = self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "class function(nn.Module):\n",
        "    def __init__(self, input_dim,output_dim, activation='relu'):\n",
        "        super(function, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "    def forward(self, e_0, h_0, h_1):\n",
        "        x = torch.cat((e_0, h_0, h_1), dim=2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional, device,method, activation='relu'):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        self.bidirectional = bidirectional\n",
        "        self.method = method\n",
        "        if self.method == '1':\n",
        "            self.lamda1 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "            self.lamda2 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "            self.lamda3 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        elif self.method == '2':\n",
        "            self.lamda1 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "            self.lamda2 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "            self.lamda3 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "        else:\n",
        "            self.func = function(input_dim*3, input_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, e_0, h_0, h_1):\n",
        "        if self.method == '1':\n",
        "            x = self.lamda1 * e_0 + self.lamda2 * h_0 + self.lamda3 * h_1\n",
        "        elif self.method == '2':\n",
        "            x = self.lamda1 * e_0 + self.lamda2 * h_0 + self.lamda3 * h_1\n",
        "        else:\n",
        "            x = self.func(e_0, h_0, h_1)\n",
        "        h0 = torch.zeros(self.n_layers * 2 if self.bidirectional else 1, x.size(0), self.hidden_dim).to(self.device)\n",
        "        c0 = torch.zeros(self.n_layers * 2 if self.bidirectional else 1, x.size(0), self.hidden_dim).to(self.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.activation(out)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def train_classifier(model, elmo_model, train_loader,val_loader, device, lr, epochs=10):\n",
        "    model.to(device)\n",
        "    elmo_model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            X, y = data\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            X_flip = torch.flip(X, [1])\n",
        "            e_f = elmo_model.embedding(X)\n",
        "            e_b = elmo_model.embedding(X_flip)\n",
        "            forward_lstm1,_ = elmo_model.lstm_forward1(e_f)\n",
        "            backward_lstm1,_ = elmo_model.lstm_backward1(e_b)\n",
        "            forward_lstm2,_ = elmo_model.lstm_forward2(forward_lstm1)\n",
        "            backward_lstm2,_ = elmo_model.lstm_backward2(backward_lstm1)\n",
        "            h_0 = torch.cat((forward_lstm1, backward_lstm1), dim=2)\n",
        "            h_1 = torch.cat((forward_lstm2, backward_lstm2), dim=2)\n",
        "            e_0 = torch.cat((e_f, e_b), dim=2)\n",
        "            y_pred = model(e_0, h_0, h_1)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        losses.append(running_loss/len(train_loader))\n",
        "\n",
        "        val_running_loss = 0.0\n",
        "        for i, data in enumerate(val_loader):\n",
        "            X, y = data\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            X_flip = torch.flip(X, [1])\n",
        "            e_f = elmo_model.embedding(X)\n",
        "            e_b = elmo_model.embedding(X_flip)\n",
        "            forward_lstm1,_ = elmo_model.lstm_forward1(e_f)\n",
        "            backward_lstm1,_ = elmo_model.lstm_backward1(e_b)\n",
        "            forward_lstm2,_ = elmo_model.lstm_forward2(forward_lstm1)\n",
        "            backward_lstm2,_ = elmo_model.lstm_backward2(backward_lstm1)\n",
        "            h_0 = torch.cat((forward_lstm1, backward_lstm1), dim=2)\n",
        "            h_1 = torch.cat((forward_lstm2, backward_lstm2), dim=2)\n",
        "            e_0 = torch.cat((e_f, e_b), dim=2)\n",
        "            y_pred = model(e_0, h_0, h_1)\n",
        "            loss = criterion(y_pred, y)\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "        val_losses.append(val_running_loss/len(val_loader))\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_running_loss/len(val_loader)}')\n",
        "\n",
        "    return losses, val_losses, model\n",
        "\n",
        "def get_predictions(model, elmomodel, data_loader, device):\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs_flip = torch.flip(inputs, [1])\n",
        "        e_f = elmomodel.embedding(inputs)\n",
        "        e_b = elmomodel.embedding(inputs_flip)\n",
        "        forward_lstm1,_ = elmomodel.lstm_forward1(e_f)\n",
        "        backward_lstm1,_ = elmomodel.lstm_backward1(e_b)\n",
        "        forward_lstm2,_ = elmomodel.lstm_forward2(forward_lstm1)\n",
        "        backward_lstm2,_ = elmomodel.lstm_backward2(backward_lstm1)\n",
        "        h_0 = torch.cat((forward_lstm1, backward_lstm1), dim=2)\n",
        "        h_1 = torch.cat((forward_lstm2, backward_lstm2), dim=2)\n",
        "        e_0 = torch.cat((e_f, e_b), dim=2)\n",
        "        outputs = model(e_0, h_0, h_1)\n",
        "        predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        ground_truth.extend(targets.argmax(dim=1).cpu().numpy())\n",
        "    return predictions, ground_truth\n",
        "\n",
        "def get_metrics(predictions, ground_truth):\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    f1 = f1_score(ground_truth, predictions, average='weighted')\n",
        "    precision = precision_score(ground_truth, predictions, average='weighted')\n",
        "    recall = recall_score(ground_truth, predictions, average='weighted')\n",
        "    cm = confusion_matrix(ground_truth, predictions)\n",
        "    return accuracy, f1, precision, recall, cm"
      ],
      "metadata": {
        "id": "RgaqJx-4HImv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'data/train.csv'\n",
        "test_data_path = 'data/test.csv'\n",
        "\n",
        "word2idx = torch.load('word2idx.pt')\n",
        "idx2word = torch.load('idx2word.pt')\n",
        "dataset = Create_dataset_classification(data_path, word2idx, idx2word)\n",
        "test_dataset = Create_dataset_classification(test_data_path, word2idx, idx2word)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "elmomodel = torch.load('model.pt',weights_only=False)"
      ],
      "metadata": {
        "id": "Lqt9Hc7nHQvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8b0aa4-a29a-4517-8f78-d9d747dea61f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-118d76a61360>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  word2idx = torch.load('word2idx.pt')\n",
            "<ipython-input-5-118d76a61360>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  idx2word = torch.load('idx2word.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save model for method 1\n",
        "model_method1 = LSTMClassifier(input_dim=300, hidden_dim=128, output_dim=dataset.num_classes, n_layers=2, bidirectional=True, device=device, method='1')\n",
        "loss_method1, val_loss_method1, model_method1 = train_classifier(model_method1, elmomodel, train_loader, val_loader, device, 0.001, 5)\n",
        "torch.save(model_method1, 'classification_model_method1.pt')\n",
        "print(\"Model for method 1 saved as 'classification_model_method1.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkj5CXcyVlBf",
        "outputId": "7aefe6fa-2ec6-47c6-b58b-346b6481a406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4037661294204493, Val Loss: 0.31051146374146144\n",
            "Epoch 2, Loss: 0.2934947016617904, Val Loss: 0.293622295593222\n",
            "Epoch 3, Loss: 0.2545347608998418, Val Loss: 0.2892797255888581\n",
            "Epoch 4, Loss: 0.2166401638649404, Val Loss: 0.30148572623233\n",
            "Epoch 5, Loss: 0.18255442553913842, Val Loss: 0.2960284621516864\n",
            "Model for method 1 saved as 'classification_model_method1.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F9NHrBtLvAdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save model for method 2\n",
        "model_method2 = LSTMClassifier(input_dim=300, hidden_dim=128, output_dim=dataset.num_classes, n_layers=2, bidirectional=True, device=device, method='2')\n",
        "loss_method2, val_loss_method2, model_method2 = train_classifier(model_method2, elmomodel, train_loader, val_loader, device, 0.001, 5)\n",
        "torch.save(model_method2, 'classification_model_method2.pt')\n",
        "print(\"Model for method 2 saved as 'classification_model_method2.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcS-7yXhWJN5",
        "outputId": "6f8c9d62-277c-4a49-de89-84f6733c846d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.40658972298726437, Val Loss: 0.32405326905846593\n",
            "Epoch 2, Loss: 0.2954547654812535, Val Loss: 0.28800793845951556\n",
            "Epoch 3, Loss: 0.2433824145719409, Val Loss: 0.2805382801989714\n",
            "Epoch 4, Loss: 0.19688450227243204, Val Loss: 0.2847843968520562\n",
            "Epoch 5, Loss: 0.15430494388192892, Val Loss: 0.31919645653665063\n",
            "Model for method 2 saved as 'classification_model_method2.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save model for method 3\n",
        "model_method3 = LSTMClassifier(input_dim=300, hidden_dim=128, output_dim=dataset.num_classes, n_layers=2, bidirectional=True, device=device, method='3')\n",
        "loss_method3, val_loss_method3, model_method3 = train_classifier(model_method3, elmomodel, train_loader, val_loader, device, 0.001, 5)\n",
        "torch.save(model_method3, 'classification_model_method3.pt')\n",
        "print(\"Model for method 3 saved as 'classification_model_method3.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_u7PBjoWLiz",
        "outputId": "68af3d96-6c17-469b-92b2-1391f3c85444"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3925129942620794, Val Loss: 0.32220088549455006\n",
            "Epoch 2, Loss: 0.2967975326317052, Val Loss: 0.2809548254013062\n",
            "Epoch 3, Loss: 0.26692185472945373, Val Loss: 0.2694308439393838\n",
            "Epoch 4, Loss: 0.24137349142382541, Val Loss: 0.3036996497809887\n",
            "Epoch 5, Loss: 0.22040396809950472, Val Loss: 0.2700006027420362\n",
            "Model for method 3 saved as 'classification_model_method3.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_dataset_classification(Dataset):\n",
        "    def __init__(self, sentences, word2idx, idx2word):\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.sentences = sentences\n",
        "        self.max_length = None\n",
        "        self.X = None\n",
        "        self.preprocess_data()\n",
        "        self.get_max_length()\n",
        "        self.padding()\n",
        "        self.create_training_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        sentences = [contractions.fix(sentence) for sentence in self.sentences]\n",
        "        sentences = [sentence.lower() for sentence in sentences]\n",
        "        sentences = [re.sub(r'http\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [re.sub(r'www\\S+', 'URL', sentence) for sentence in sentences]\n",
        "        sentences = [sentence.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for sentence in sentences]\n",
        "        sentences = [(sentence.split()) for sentence in sentences]\n",
        "        sentences = [['<s>'] + sentence + ['</s>'] for sentence in sentences]\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def get_max_length(self):\n",
        "        sentence_lengths = [len(sentence) for sentence in self.sentences]\n",
        "        self.max_length = int(np.percentile(sentence_lengths, 95))\n",
        "\n",
        "    def padding(self):\n",
        "        padded_sentences = []\n",
        "        for sentence in self.sentences:\n",
        "            padded_sentence = [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in sentence]\n",
        "            if len(padded_sentence) < self.max_length:\n",
        "                padded_sentence += [self.word2idx['<pad>']] * int(self.max_length - len(padded_sentence))\n",
        "                padded_sentences.append(padded_sentence)\n",
        "            else:\n",
        "                padded_sentences.append(padded_sentence[:self.max_length])\n",
        "\n",
        "        self.sentences = padded_sentences\n",
        "\n",
        "    def create_training_data(self):\n",
        "        X = []\n",
        "        for sentence in self.sentences:\n",
        "            X.append(sentence)\n",
        "\n",
        "        self.X = torch.tensor(X)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx]\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional, device, method, activation='relu'):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        self.bidirectional = bidirectional\n",
        "        self.method = method\n",
        "        if self.method == '1':\n",
        "            self.lamda1 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "            self.lamda2 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "            self.lamda3 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        elif self.method == '2':\n",
        "            self.lamda1 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "            self.lamda2 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "            self.lamda3 = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "        else:\n",
        "            self.func = function(input_dim*3, input_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, e_0, h_0, h_1):\n",
        "        if self.method == '1':\n",
        "            x = self.lamda1 * e_0 + self.lamda2 * h_0 + self.lamda3 * h_1\n",
        "        elif self.method == '2':\n",
        "            x = self.lamda1 * e_0 + self.lamda2 * h_0 + self.lamda3 * h_1\n",
        "        else:\n",
        "            x = self.func(e_0, h_0, h_1)\n",
        "        h0 = torch.zeros(self.n_layers * 2 if self.bidirectional else 1, x.size(0), self.hidden_dim).to(self.device)\n",
        "        c0 = torch.zeros(self.n_layers * 2 if self.bidirectional else 1, x.size(0), self.hidden_dim).to(self.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.activation(out)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python inference.py <saved model path> <description>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    model_path = 'classification_model_method1.pt'\n",
        "    description = 'Unions representing workers at Turner Newall say they are disappointed after talks with stricken parent firm Federal Mogul.'\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load the saved models and word2idx\n",
        "    word2idx = torch.load('word2idx.pt')\n",
        "    idx2word = torch.load('idx2word.pt')\n",
        "    elmo_model = torch.load('model.pt',weights_only=False)\n",
        "    classifier_model = torch.load(model_path,weights_only=False)\n",
        "\n",
        "    # Preprocess the input description\n",
        "    dataset = Create_dataset_classification([description], word2idx, idx2word)\n",
        "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Get the predictions\n",
        "    classifier_model.eval()\n",
        "    elmo_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X in data_loader:\n",
        "            X = X.to(device)\n",
        "            X_flip = torch.flip(X, [1])\n",
        "            e_f = elmo_model.embedding(X)\n",
        "            e_b = elmo_model.embedding(X_flip)\n",
        "            forward_lstm1, _ = elmo_model.lstm_forward1(e_f)\n",
        "            backward_lstm1, _ = elmo_model.lstm_backward1(e_b)\n",
        "            forward_lstm2, _ = elmo_model.lstm_forward2(forward_lstm1)\n",
        "            backward_lstm2, _ = elmo_model.lstm_backward2(backward_lstm1)\n",
        "            h_0 = torch.cat((forward_lstm1, backward_lstm1), dim=2)\n",
        "            h_1 = torch.cat((forward_lstm2, backward_lstm2), dim=2)\n",
        "            e_0 = torch.cat((e_f, e_b), dim=2)\n",
        "            outputs = classifier_model(e_0, h_0, h_1)\n",
        "            probabilities = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        print(f'class-{i+1} {prob:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6hU75L2UHG8",
        "outputId": "8b37eae8-d9f3-4a53-de40-7e1f25cf9808"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class-1 0.3386\n",
            "class-2 0.0001\n",
            "class-3 0.3957\n",
            "class-4 0.2657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7609a1cd9034>:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  word2idx = torch.load('word2idx.pt')\n",
            "<ipython-input-11-7609a1cd9034>:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  idx2word = torch.load('idx2word.pt')\n"
          ]
        }
      ]
    }
  ]
}